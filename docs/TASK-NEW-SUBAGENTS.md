# Tasks: Material Sub-Agents + Audio-Features + Fundament

## Reihenfolge (WICHTIG)
**Erst Fundament (Tasks 0a-0g), dann neue Agents (Tasks 1-9), dann Audio (Tasks 10-12).**
Ohne Fundament arbeiten alle Agents "blind" â€” keine Good Practices, keine Preferences, kein Chat-Kontext.

## Design-Prinzipien (NEU)
1. **Prompts radikal vereinfachen**: Nur Kern (Struktur, QualitÃ¤tskriterien, Output-Format). Tiefe kommt aus Wissenskarte (DB).
2. **Kontextdaten statt Template-Variablen**: Lernfeld, Halbjahr, Fach etc. aus Konversation + Profil + Curriculum-RAG â€” nicht im Prompt hardcoden.
3. **Ginger-Logik eliminieren**: Keine if/else im Prompt fÃ¼r Schulform/Beruf â€” der Agent zieht sich den Kontext selbst.
4. **Ãœberarbeitbarkeit als Grundprinzip**: Jedes Material strukturiert speichern (Teile, nicht Blob), damit "Ã¤ndere Aufgabe 2" funktioniert.
5. **Multi-Turn bei ALLEN Agents**: Two-Phase Pattern (`generate` â†’ `continue`) + `agent_sessions` State.
6. **SchÃ¤rfungsfragen vor Generierung**: Jeder Agent fragt erst relevante Parameter ab.
7. **Implicit Feedback Loop**: Download/Iteration = positiv â†’ `agent_knowledge` rating.

---

## Task 0: Fundament â€” agent_knowledge + Wissenskarte

### 0a: Tabelle `agent_knowledge` in Supabase erstellen
- Schema aus `docs/AGENT-ARCHITEKTUR-V2.md` (Zeile 80-115)
- Indices fÃ¼r teacher_id, fach, agent_type, quality_score, embedding
- RLS Policies

### 0b: Wissenskarte-Funktion bauen
- `_build_wissenskarte(teacher_id, agent_type, fach)` â†’ kompakter Text (~100-200 Tokens)
- Aggregiert: Generic Profile + Anzahl Examples + Anzahl Good Practices + Preferences
- Wird in Sub-Agent System-Prompt injiziert

### 0c: Lazy-Loading Tools fÃ¼r Sub-Agents
- `get_good_practices(fach, thema, limit)` â†’ pgvector Similarity Search in agent_knowledge
- `get_full_context(conversation_id)` â†’ Chat-Verlauf zusammenfassen (Summary, nicht raw!)
- `get_example(upload_id)` â†’ Lehrer-Upload laden

### 0d: Generic Profiles seeden
- Aus gesammelten Prompts (4 vorhanden: Lernsituation, Lernspiel, Versuchsanleitung, Stundenplanung)
- Mystery: Eigenen Prompt entwerfen (Recherche-basiert)
- Klausur, Differenzierung: Aus bestehendem Code extrahieren
- Je 1 Eintrag: agent_type + fach + knowledge_type="generic" + content=JSON
- **RADIKAL VEREINFACHT**: Nur Kern-QualitÃ¤tskriterien, keine Template-Logik

### 0e: Bestehenden Klausur-Agent upgraden (Pilot)
- Von Haiku â†’ Sonnet
- Wissenskarte in System-Prompt einbauen
- Good Practice Tool anbinden
- Full Context Tool anbinden
- **Strukturierte Speicherung**: Aufgaben einzeln speicherbar fÃ¼r Ãœberarbeitung
- Testen ob QualitÃ¤t steigt

### 0f: Knowledge Cleanup Job
- Max ~50 EintrÃ¤ge pro teacher_id + agent_type
- Archivierung: quality_score < 0.3 AND times_used = 0 AND Ã¤lter 90 Tage
- ZusammenfÃ¼hrung: Doppelte Preferences mergen
- Cron: 1x/Woche oder bei jedem Material-Generate prÃ¼fen

### 0g: material_learning_agent.py umbauen
- Aktuell: Zeigt auf nicht-existierende Tabellen â†’ schlÃ¤gt still fehl
- Neu: Auf `agent_knowledge` umstellen (eine Tabelle statt drei)
- Implizites Rating: Download/Iteration = positiv â†’ quality_score erhÃ¶hen

---

## Task 1: Hilfekarten-Agent
**Prompt**: Eigener Entwurf (Differenzierungsexperte)
**Model**: Haiku (einfache Struktur)
- Kompakte 1-seitige Hilfekarten
- Schrittweise Hilfen + Beispiele + Tipps
- Niveau: Basis / Erweitert / Experte
- DOCX-Output

## Task 2: Escape-Room-Agent
**Prompt**: Eigener Entwurf (RÃ¤tsel-Designer)
**Model**: Sonnet (komplex, aufeinander aufbauende RÃ¤tsel)
- Story/Narrative + verkettete RÃ¤tsel
- Optional: KI-Bildgenerierung fÃ¼r visuelle RÃ¤tsel
- Analog/Digital-Varianten
- DOCX-Output + optional H5P

## Task 3: Mystery-Agent
**Prompt**: Eigener Entwurf (Recherche-basiert, kein alter Prompt vorhanden)
**Model**: Sonnet (Informationskarten-Design komplex)
- Leitfrage â†’ Informationskarten â†’ AuflÃ¶sung
- Kategorien: Fakt / Hinweis / IrrefÃ¼hrung
- Differenzierung durch Kartensets
- DOCX-Output

## Task 4: Lernsituation-Agent
**Prompt**: Vereinfacht aus `docs/prompts/lernsituation-pflege.md`
**Model**: Sonnet (berufliche Bildung, Curriculum-intensiv)
- Handlungsorientierung, Lernfelder, Kompetenzbereiche
- Starke Curriculum-RAG Nutzung
- Fallbeispiel-Generator integriert

## Task 5: Lernspiel-Agent
**Prompt**: Vereinfacht aus empfangenem Prompt (file_65)
**Model**: Haiku (kreativ aber strukturiert)
- Kreative Lernspiele fÃ¼r alle Schulformen
- HTML-Output mit Spielname, Regeln, Inhalt, Material, Varianten
- Kompetenzbereich-sensitiv

## Task 6: Versuchsanleitung-Agent (Arbeitsblatt)
**Prompt**: Vereinfacht aus empfangenem Prompt (file_66)
**Model**: Haiku (strukturiert)
- Grad der Offenheit: Geschlossen / Gelenkt / Offen
- Protokollstruktur: Frei / LÃ¼ckentext / Tabellen
- Sprachniveau-Differenzierung (spÃ¤ter Ã¼ber Lerngruppen)
- DOCX-Output

## Task 7: Stundenplanungs-Agent
**Prompt**: Vereinfacht aus empfangenem Prompt (file_67)
**Model**: Sonnet (Verlaufsplan-Tabellen komplex)
- Einzelstunde / Doppelstunde / Unterrichtsreihe
- Verlaufsplan: Zeit | Phasen | LK-Verhalten | SuS-Verhalten | Sozialformen | Medien
- Handlungsorientierung als didaktisches Prinzip
- DOCX-Output

---

## Task 8: YouTube-Quiz
**Engine**: gemini
- YouTube Transcript Extraction (yt-dlp)
- Quiz-Agent: MultiChoice, True/False, LÃ¼ckentext
- Output: DOCX oder H5P (BrÃ¼cke zu H5P-Agent)
- Frontend: YouTube-URL im Chat â†’ automatische Erkennung

## Task 9: Text-to-Speech (Vorlesen)
**Phase 1**: Web Speech API (kostenlos, sofort)
- ðŸ”Š Button an jeder Assistant-Nachricht
- Deutsche Stimme

**Phase 2**: ElevenLabs TTS
- Bessere QualitÃ¤t, natÃ¼rlichere Stimmen
- `POST /api/tts` â†’ Audio â†’ Frontend

---

## Task 10: GesprÃ¤chssimulations-Agent (ElevenLabs)
**API**: ElevenLabs Conversational AI / TTS
**Model**: Sonnet (Szenario-Design) + ElevenLabs (Audio)
- Patienten-/KundengesprÃ¤ch fÃ¼r berufliche Bildung
- Multi-Voice Dialog (verschiedene Sprecher)
- Eigener Agent mit SchÃ¼ler-Freigabe (Access-Code Pattern wie H5P)
- RÃ¼ckfragen: GesprÃ¤chstyp, Rollen, Schwierigkeitsgrad, Lernziel
- **Ãœberarbeitbar**: Einzelne GesprÃ¤chsteile Ã¤nderbar
- Benchmark: GesprÃ¤chsrealismus, didaktischer Wert

## Task 11: Podcast-Agent (ElevenLabs)
**API**: ElevenLabs TTS (Multi-Voice)
**Model**: Sonnet (Skript) + ElevenLabs (Audio)
- Aus Unterrichtsinhalten einen Podcast generieren
- Multi-Voice (Moderator + Experte / Dialog-Format)
- RÃ¼ckfragen: Thema, Zielgruppe, Dauer, Format (Monolog/Dialog)
- **Ãœberarbeitbar**: Skript-Abschnitte einzeln Ã¤nderbar
- DOCX-Skript + Audio-Datei als Output
- Benchmark: Audio-QualitÃ¤t, inhaltliche Korrektheit

## Task 12: Benchmark-Erweiterung
- Bestehende 18 Tests â†’ erweitern um:
  - Material-QualitÃ¤t pro Agent-Typ
  - Ãœberarbeitungs-Workflow (Iteration)
  - Audio-QualitÃ¤t (TTS, Podcast, GesprÃ¤chssimulation)
  - SchÃ¤rfungsfragen-QualitÃ¤t
  - Wissenskarte-Effekt (mit vs. ohne)
- Gemini als Cross-Validator (wie SCL-Tests)
- Automatisierte Regression nach jedem Deploy
