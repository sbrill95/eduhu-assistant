# TODOs fÃ¼r 18.02.2026

Vorbereitet von Claudius ðŸ¦‘ fÃ¼r Steffen & Christopher.
Reviewed von Gemini 2.5 Pro â€” Feedback eingearbeitet.
Update 08:16: Steffens Klarstellungen eingearbeitet.

---

## 1. ðŸ“Š Token-Tracking pro Benutzer â€” PrioritÃ¤t: SEHR HOCH

**Ziel:** Ãœbersicht welcher Nutzer welches Modell wie oft/viel nutzt + Kosten. Sichtbar im Profil (Steffen, Christopher, Micha).

### Datenmodell
```sql
CREATE TABLE token_usage (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    teacher_id UUID REFERENCES teachers(id),
    model TEXT NOT NULL,              -- z.B. 'claude-sonnet-4', 'claude-3-5-haiku'
    input_tokens INTEGER DEFAULT 0,
    output_tokens INTEGER DEFAULT 0,
    cost_usd NUMERIC(10, 6),         -- âš¡ Gemini: Kosten sofort mitberechnen!
    agent_type TEXT,                  -- 'main', 'klausur', 'memory', etc.
    request_id UUID,                  -- Anfragen gruppieren
    created_at TIMESTAMPTZ DEFAULT now()
);

CREATE INDEX idx_token_usage_teacher ON token_usage(teacher_id);
CREATE INDEX idx_token_usage_date ON token_usage(created_at);
```

### Preistabelle (hartcodiert, als Dict im Backend)
```python
MODEL_PRICES = {  # USD pro 1M Tokens
    "claude-sonnet-4": {"input": 3.0, "output": 15.0},
    "claude-3-5-haiku": {"input": 0.80, "output": 4.0},
}
```

### Backend
- **Logging via `background_task`** (nicht Fire-and-forget!) â€” FastAPI Background Tasks mit Retry
- Pydantic AI gibt `result.usage()` zurÃ¼ck mit `input_tokens`, `output_tokens`
- Kosten bei Insert berechnen: `cost = (input * price_in / 1M) + (output * price_out / 1M)`
- Erfasst ALLE Agent-Calls: Main, Sub-Agents (Klausur, Memory, Curriculum, etc.)
- **Auch fehlgeschlagene Calls tracken** (Tokens werden trotzdem verbraucht!)

### API Endpoints
```
GET /api/profile/token-usage?days=7&agent_type=klausur  â†’ Tageweise, filterbar
GET /api/profile/token-usage?days=30                     â†’ MonatsÃ¼bersicht
```

Response:
```json
{
  "daily": [
    { "date": "2026-02-18", "model": "claude-sonnet-4", "input_tokens": 12400, "output_tokens": 8200, "calls": 15, "cost_usd": 0.16 }
  ],
  "total": { "input_tokens": 15600, "output_tokens": 10000, "calls": 23, "cost_usd": 0.20 }
}
```

### Frontend (ProfilePage)
- Neue Section "Token-Verbrauch" im Profil
- **TagesÃ¼bersicht** als Hauptansicht (Steffen: "TagesÃ¼bersicht wÃ¤re wichtig")
- Tabelle: Datum | Modell | Input | Output | Kosten | Calls
- Filterbar nach `agent_type`
- Optional: Kleines Balkendiagramm pro Tag (CSS-only)

### Aufwand: ~4-6h (korrigiert nach Gemini-Review)
- DB + RLS Policies: 30 min
- Backend Hook + Retry + Preisberechnung: 1.5h
- API mit Aggregation + Filtern: 45 min
- Frontend: 1.5h
- Testing: 1h

### âš ï¸ Risiken (Gemini)
- Performance bei groÃŸer Tabelle â†’ spÃ¤ter MATERIALIZED VIEW oder Archivierung
- UnvollstÃ¤ndiges Tracking bei Timeouts/AbbrÃ¼chen

### AufschlÃ¼sselung (Steffen-Klarstellung)
- **Nach Modell**: claude-sonnet-4 vs. claude-3-5-haiku getrennt
- **Nach Agent-Typ**: Haupt-Agent vs. Sub-Agents (memory, klausur, curriculum, etc.)
- **ZeitrÃ¤ume**: Heute / Letzte 7 Tage / Letzte 30 Tage (3 Tabs oder Toggle)
- ~~Budget-Alerts~~ â†’ nicht nÃ¶tig, Credit-Problem wird nicht mehr vorkommen

---

## 2. ðŸ”˜ RÃ¼ckfrage-Cards mit Klick-Buttons â€” PrioritÃ¤t: MITTEL

**Ziel:** Wenn der Agent Optionen zur Auswahl gibt â†’ klickbare Buttons statt FlieÃŸtext.

### Ist-Zustand âœ…
- `ClarificationCard.tsx` existiert
- `ask_teacher` Tool mit `options`-Feld existiert
- Frontend zeigt Optionen als Buttons
- Klick â†’ Text-Nachricht â†’ funktioniert

### Was zu tun ist
1. **Konsistenz-Regel**: "Jede binÃ¤re oder Multiple-Choice-Frage an den User MUSS Ã¼ber `ask_teacher` mit `options` laufen" â€” als Prompt-Snippet in `base.py`
2. **Alle 12 Agents durchgehen**: PrÃ¼fen ob RÃ¼ckfragen `options` setzen
3. **Styling verbessern**: Buttons prominenter, ggf. Icons
4. **Fallback**: Wenn User trotzdem Text tippt statt zu klicken â†’ muss trotzdem funktionieren
5. **Benchmark-Tests fÃ¼r Cards**: Testsuite um RÃ¼ckfrage-Card-Tests erweitern (prÃ¼fen ob `options` Array in Response)

### ~~Multi-Step Wizard~~ â†’ Eigenes Epic (Gemini-Empfehlung)
Aus diesem Task gestrichen. Erfordert State Machine + eigenes Konzept.

### Referenz: Claude.ai
"Suggested replies" als Chips. Unser `ClarificationNeededError` + `options` ist analog.

### Aufwand: ~1-2h
- Prompt-Snippet + Agents anpassen: 30 min
- Frontend Polish: 30 min
- Testing: 30 min

---

## 3. â³ Agent-Arbeitsanzeige (Pulsing Indicator) â€” PrioritÃ¤t: MITTEL

**Ziel:** User sieht DASS der Agent arbeitet. Nicht stiller Textstream.

### Ist-Zustand
- SSE Streaming existiert âœ…
- Tool-Call-Schritte als `[ðŸ”§ Tool wird ausgefÃ¼hrt...]` âœ…
- Material-Generierung: Agent denkt lange, dann alles auf einmal âŒ

### GewÃ¼nschtes Verhalten
1. **Schritt-Anzeige**: "ðŸ” Analysiere Anforderungen..." â†’ "ðŸ“ Erstelle Aufgaben..." â†’ "âœ… Fertig!"
2. **Pulsing-Animation**: CSS-Animation sichtbar
3. **Ergebnis in eigener Nachricht**: Nicht inline im Stream

### Technische Umsetzung
- SSE Events erweitern: `event: step`
  ```json
  { "step_name": "Erstelle Klausur...", "current_step": 2, "total_steps": 3 }
  ```
  *(Gemini: Keine Prozent-Anzeige â€” LLMs denken nicht linear. Step-Counter stattdessen.)*
- Frontend: `StepIndicator.tsx` mit Pulse-Animation
- **V1: Statische Schritte** ("Rufe Agent auf...", "Formatiere...") â€” reicht fÃ¼r den Anfang
- Material-Ergebnis als eigene "Nachricht" im Chat

### Aufwand: ~2-3h
- Backend SSE Steps (statisch): 1h
- Frontend StepIndicator: 1h
- Integration + Test: 1h

---

## 4. ðŸ§  Memory-System Review & Restructuring â€” PrioritÃ¤t: HOCH

**Ziel:** Memory analysieren, aufrÃ¤umen, fÃ¼r User sichtbar machen, STM/LTM explizit trennen.

### Ist-Zustand
- **Tabelle**: `user_memories` (Supabase)
- **Felder**: `id, teacher_id, category, key, value, created_at, updated_at, access_count`
- **~231 EintrÃ¤ge** (nach Dedup von 266) fÃ¼r Steffen
- **Top 50** per Recency+Relevancy im System-Prompt (~854 Tokens)
- **Memory-Agent** (Haiku): Fire-and-forget nach jeder Nachricht
- **Problem**: Massive Duplikation

### Phase 1: Analyse (SQL zuerst!)
```sql
-- Memories pro Teacher
SELECT teacher_id, COUNT(*) FROM user_memories GROUP BY teacher_id;

-- Kategorien-Verteilung
SELECT category, COUNT(*) FROM user_memories 
WHERE teacher_id = 'STEFFEN_ID' GROUP BY category ORDER BY count DESC;

-- Duplikate
SELECT value, COUNT(*) as cnt FROM user_memories 
WHERE teacher_id = 'STEFFEN_ID' GROUP BY value HAVING COUNT(*) > 1;

-- Durchschnittliches Alter
SELECT AVG(EXTRACT(EPOCH FROM (now() - created_at))/86400) as avg_days 
FROM user_memories WHERE teacher_id = 'STEFFEN_ID';
```

### Phase 2: STM/LTM Modell (Christopher)

| Aspekt | KurzzeitgedÃ¤chtnis (STM) | LangzeitgedÃ¤chtnis (LTM) |
|--------|--------------------------|--------------------------|
| Quelle | Letzte ~30 Nachrichten (Chat History) | `user_memories` Tabelle |
| Inhalt | Aktueller Kontext, laufende Aufgaben | Preferences, Fakten, Gewohnheiten |
| Lebensdauer | Session-basiert | Permanent (mit Decay) |
| Im Prompt | Immer | Top-N nach Relevanz+Recency |
| Verwaltung | Automatisch | Memory-Agent + Cleanup |

**Gemini**: De facto existiert dieses Modell bereits. Explizit machen + dokumentieren.

### Phase 3: Kategorien â€” DYNAMISCH aus DB (Steffen-Klarstellung)
Keine festen Kategorien im Code! Stattdessen:
- Kategorien aus `user_memories.category` dynamisch ableiten
- PrÃ¼fen: Welche Kategorien existieren aktuell in der DB?
```sql
SELECT DISTINCT category FROM user_memories WHERE teacher_id = 'STEFFEN_ID';
```
- Frontend gruppiert nach vorhandenen Kategorien
- Neue Kategorien entstehen automatisch durch Memory-Agent

### Phase 3b: KurzzeitgedÃ¤chtnis â€” System-Prompt-Logik dokumentieren
- **Was kommt rein?** Top 50 Memories, sortiert nach Recency+Relevancy Score
- **Scoring**: `score = relevancy * 0.7 + recency * 0.3` (oder Ã¤hnlich)
- **Transparenz**: Dem User zeigen welche Memories gerade "aktiv" sind
- Sind die Kategorien die der Memory-Agent nutzt konsistent? Welche gibt es?

### Phase 4: User-Facing Memory View â€” NUR LÃ–SCHEN (Steffen-Klarstellung)
```
GET /api/profile/memories              â†’ Alle Memories, nach Kategorie gruppiert
DELETE /api/profile/memories/:id       â†’ Einzelne Memory lÃ¶schen
```
**KEIN Editieren!** Nur lesen + lÃ¶schen. (Steffen: "nicht editierbar, sondern nur lÃ¶schbar")

Frontend: Section im Profil oder eigene Seite
- Grouped by Category (dynamisch aus DB)
- Jede Memory lÃ¶schbar (MÃ¼lleimer-Icon)
- **"Was weiÃŸ die KI Ã¼ber mich?"** â†’ Transparenz + Vertrauen + Kontrolle

### Phase 5: Cleanup â€” Batch nach jeder Conversation (Steffen-Klarstellung)
~~Cron-Job 3x/Tag~~ â†’ **Batch-Cleanup nach jeder abgeschlossenen Conversation**
- Trigger: Wenn Conversation endet (z.B. 30min InaktivitÃ¤t oder neue Conversation)
- Batch-API: Duplikate mergen, veraltete lÃ¶schen, Ã¤hnliche zusammenfassen
- **Admin-Endpoint** bleibt als manueller Fallback
- ~~Inline-Cleanup~~ (Gemini: erhÃ¶ht Latenz, vermeiden)

### Aufwand: ~5-6h (korrigiert nach Gemini-Review)
- Analyse (SQL): 30 min
- API Endpoints: 1h
- Frontend Memory-View: 2h
- Cleanup-Logik (die eigentliche KomplexitÃ¤tsbombe): 1.5h
- Testing: 1h

---

## Zusammenfassung

| # | Feature | Aufwand | PrioritÃ¤t |
|---|---------|---------|-----------|
| 1 | Token-Tracking + Kosten | 4-6h | ðŸ”´ Sehr Hoch |
| 2 | RÃ¼ckfrage-Buttons | 1-2h | ðŸŸ¡ Mittel |
| 3 | Agent-Arbeitsanzeige | 2-3h | ðŸŸ¡ Mittel |
| 4 | Memory-Restructuring | 5-6h | ðŸŸ  Hoch |

**Gesamt: ~12-17h Arbeit** (realistischer als die ursprÃ¼nglichen 8-12h)

### Empfohlene Reihenfolge
1. **Token-Tracking** â€” Kostentransparenz sofort (Ã¼berlebenswichtig)
2. **Memory-Analyse** (Phase 1+2) â€” Datengrundlage schaffen
3. **RÃ¼ckfrage-Buttons** â€” Quick Win
4. **Agent-Arbeitsanzeige** â€” UX Polish
5. **Memory-View + Cleanup** (Phase 3-5) â€” nach Analyse

### Benchmark-Ergebnisse (Referenz)

**Full Suite (mit LLM Judge): 44/62 (71%)**
- 10 Fails = Anthropic Credits aufgebraucht (500er ab Test ~45)
- 3 Fails = Judge zu streng (2/5 fÃ¼r "formale SchwÃ¤chen")
- 1 Fail = Judge JSON-Parse-Error
- 4 echte Fails: J07.2, J11.4, J12.x
- **Ohne Credit-AusfÃ¤lle: ~54/62 (87%)**

**Medium Suite: 28/30 (93%)** âœ…
- J01 Klausur: 5/5 âœ…
- J02 Differenzierung: 2/3 (Assertion zu eng)
- J03 H5P: 2/4 (Assertion zu eng)
- J04 Lehrplan: 3/3 âœ…
- J05 Stundenplanung: 3/3 âœ…
- J06 Memory: 3/3 âœ…
- J11 Multi-Turn: 4/4 âœ…
- J13 Todos: 2/2 âœ…
- QualitÃ¤t: 3/3 âœ…
